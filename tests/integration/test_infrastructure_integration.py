"""
Testes de Integra√ß√£o Completa - EVAonline Infrastructure
==========================================================

Este m√≥dulo testa a integra√ß√£o completa entre:
- PostgreSQL (banco de dados geoespacial com PostGIS)
- Redis (cache e message broker)
- APIs externas (OpenMeteo)
- Sistema de mapas (MATOPIBA e World Map)
- Aplica√ß√£o Dash (frontend)

Requisitos:
- Docker Desktop instalado e rodando
- PostgreSQL instalado (ou container ativo)
- Redis instalado (ou container ativo)
- Arquivo .env configurado corretamente

Para executar:
    pytest tests/integration/test_infrastructure_integration.py -v --tb=short
"""

import os
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Tuple

import geopandas as gpd
import pandas as pd
import psycopg  # psycopg3 - melhor suporte Unicode no Windows
import pytest
import redis
from loguru import logger
from sqlalchemy import create_engine, text
from sqlalchemy.exc import OperationalError

# Adicionar backend ao path
backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))

from backend.database.connection import DATABASE_URL, engine, get_db_context
from backend.infrastructure.clients.elevation_api import get_openmeteo_elevation

# ============================================================================
# FIXTURES
# ============================================================================

@pytest.fixture(scope="session")
def redis_client():
    """
    Fixture para cliente Redis com configura√ß√£o de m√∫ltiplos ambientes.
    Tenta conectar em: Docker -> Local -> CI/CD
    """
    # Tentar diferentes configura√ß√µes
    redis_configs = [
        # Docker Compose (container name)
        {"host": "redis", "port": 6379, "password": "evaonline", "db": 0},
        # Docker Compose (localhost com password)
        {"host": "localhost", "port": 6379, "password": "evaonline", "db": 0},
        # Redis local sem senha
        {"host": "localhost", "port": 6379, "password": None, "db": 0},
        # Redis local com senha padr√£o
        {"host": "127.0.0.1", "port": 6379, "password": "evaonline", "db": 0},
    ]
    
    client = None
    for config in redis_configs:
        try:
            logger.info(f"Tentando conectar ao Redis: {config['host']}:{config['port']}")
            client = redis.Redis(
                host=config["host"],
                port=config["port"],
                password=config["password"],
                db=config["db"],
                decode_responses=True,
                socket_timeout=5,
                socket_connect_timeout=5
            )
            # Testar conex√£o
            client.ping()
            logger.success(f"‚úÖ Conectado ao Redis: {config['host']}:{config['port']}")
            break
        except redis.ConnectionError as e:
            logger.warning(f"‚ùå Falha ao conectar: {config['host']}:{config['port']} - {e}")
            continue
    
    if client is None:
        pytest.skip("Redis n√£o est√° dispon√≠vel em nenhuma configura√ß√£o testada")
    
    yield client
    
    # Cleanup: Limpar chaves de teste
    try:
        test_keys = client.keys("test:*")
        if test_keys:
            client.delete(*test_keys)
            logger.info(f"üßπ Limpeza: {len(test_keys)} chaves de teste removidas")
    except Exception as e:
        logger.warning(f"Erro na limpeza do Redis: {e}")


@pytest.fixture(scope="session")
def postgres_connection():
    """
    Fixture para conex√£o PostgreSQL com psycopg3 (melhor suporte Unicode).
    """
    # Tentar diferentes configura√ß√µes (priorizar localhost para Docker local)
    pg_configs = [
        # PostgreSQL local via localhost (Docker Desktop ou Docker Compose)
        {
            "host": "localhost",
            "port": 5432,
            "user": "evaonline",
            "password": "evaonline",
            "dbname": "evaonline"
        },
        # PostgreSQL local (instala√ß√£o Windows nativa)
        {
            "host": "localhost",
            "port": 5432,
            "user": "postgres",
            "password": "postgres",
            "dbname": "evaonline"
        },
        # Docker Compose (dentro de container)
        {
            "host": "postgres",
            "port": 5432,
            "user": "evaonline",
            "password": "evaonline",
            "dbname": "evaonline"
        }
    ]
    
    conn = None
    for config in pg_configs:
        try:
            logger.info(
                f"Tentando conectar ao PostgreSQL: "
                f"{config['host']}:{config['port']}"
            )
            # Conectar com psycopg3 (melhor que psycopg2 no Windows)
            conn = psycopg.connect(**config)
            
            # Testar conex√£o
            cursor = conn.cursor()
            cursor.execute("SELECT version()")
            version = cursor.fetchone()[0]
            cursor.close()
            
            logger.success(f"Conectado ao PostgreSQL: {version[:50]}...")
            yield conn
            conn.close()
            return
        except Exception as e:
            logger.warning(f"Falha ao conectar: {e}")
            if conn:
                conn.close()
            continue
    
    pytest.skip(
        "PostgreSQL n√£o est√° dispon√≠vel em nenhuma configura√ß√£o testada"
    )


@pytest.fixture(scope="session")
def sample_locations():
    """Coordenadas de teste para diferentes regi√µes."""
    return {
        "ja√∫_sp": {"lat": -22.2964, "long": -48.5578, "name": "Ja√∫, SP"},
        "bras√≠lia_df": {"lat": -15.7942, "long": -47.8822, "name": "Bras√≠lia, DF"},
        "cuiab√°_mt": {"lat": -15.5989, "long": -56.0949, "name": "Cuiab√°, MT"},
        "palmas_to": {"lat": -10.1840, "long": -48.3336, "name": "Palmas, TO"},
        "s√£o_paulo_sp": {"lat": -23.5505, "long": -46.6333, "name": "S√£o Paulo, SP"}
    }


@pytest.fixture
def geospatial_files():
    """Paths para arquivos geoespaciais do projeto."""
    base_dir = Path(__file__).parent.parent.parent
    return {
        "brasil": base_dir / "data" / "geojson" / "BR_UF_2024.geojson",
        "matopiba": base_dir / "data" / "geojson" / "Matopiba_Perimetro.geojson",
        "cities": base_dir / "data" / "csv" / "CITIES_MATOPIBA_337.csv"
    }


# ============================================================================
# TESTES DE CONECTIVIDADE
# ============================================================================

class TestConnectivity:
    """Testes de conectividade b√°sica com infraestrutura."""
    
    def test_redis_ping(self, redis_client):
        """Testa se o Redis responde ao comando PING."""
        assert redis_client.ping() is True
        logger.success("‚úÖ Redis PING: OK")
    
    def test_postgres_connection(self, postgres_connection):
        """Testa se o PostgreSQL est√° acess√≠vel."""
        cursor = postgres_connection.cursor()
        cursor.execute("SELECT 1")
        result = cursor.fetchone()
        cursor.close()
        assert result[0] == 1
        logger.success("‚úÖ PostgreSQL Connection: OK")
    
    def test_postgres_postgis_extension(self, postgres_connection):
        """Verifica se a extens√£o PostGIS est√° instalada."""
        cursor = postgres_connection.cursor()
        cursor.execute("SELECT PostGIS_version()")
        result = cursor.fetchone()
        cursor.close()
        version = result[0]
        # PostGIS retorna apenas a vers√£o (ex: "3.4 USE_GEOS=1...")
        assert version is not None and len(version) > 0
        logger.success(f"PostGIS Extension: {version}")
    
    def test_database_tables_exist(self, postgres_connection):
        """Verifica se as tabelas principais existem."""
        cursor = postgres_connection.cursor()
        cursor.execute("""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = 'public'
            AND table_type = 'BASE TABLE'
        """)
        tables = [row[0] for row in cursor.fetchall()]
        cursor.close()
        logger.info(f"üìä Tabelas encontradas: {tables}")
        # N√£o falhar se n√£o houver tabelas (banco novo)
        logger.success(f"‚úÖ Database Schema: {len(tables)} tabela(s)")


# ============================================================================
# TESTES DE CACHE REDIS
# ============================================================================

class TestRedisCache:
    """Testes de funcionalidades de cache Redis."""
    
    def test_set_get_string(self, redis_client):
        """Testa opera√ß√µes b√°sicas SET/GET."""
        key = "test:string"
        value = "EVAonline Test"
        
        redis_client.set(key, value)
        retrieved = redis_client.get(key)
        
        assert retrieved == value
        logger.success(f"‚úÖ Redis SET/GET: {key} = {value}")
    
    def test_set_with_expiry(self, redis_client):
        """Testa SET com TTL (expira√ß√£o)."""
        key = "test:expiry"
        value = "temporary_data"
        ttl = 5  # 5 segundos
        
        redis_client.setex(key, ttl, value)
        
        # Verificar que existe
        assert redis_client.get(key) == value
        
        # Verificar TTL
        remaining = redis_client.ttl(key)
        assert 0 < remaining <= ttl
        
        logger.success(f"‚úÖ Redis SETEX: TTL={remaining}s")
    
    def test_cache_elevation_data(self, redis_client, sample_locations):
        """Testa cache de dados de eleva√ß√£o (padr√£o do projeto)."""
        location = sample_locations["ja√∫_sp"]
        cache_key = f"test:elevation:{location['lat']}:{location['long']}"
        elevation_data = {
            "elevation": 542.5,
            "lat": location['lat'],
            "long": location['long'],
            "timestamp": datetime.now().isoformat()
        }
        
        # Simular cache com 24h TTL (padr√£o do projeto)
        redis_client.setex(
            cache_key,
            86400,  # 24 hours
            str(elevation_data)
        )
        
        # Verificar
        cached = redis_client.get(cache_key)
        assert cached is not None
        assert str(location['lat']) in cached
        
        logger.success(f"‚úÖ Elevation Cache: {cache_key}")
    
    def test_cache_hit_miss_metrics(self, redis_client):
        """Testa padr√£o de cache hit/miss para m√©tricas."""
        base_key = "test:metrics"
        
        # Simular cache miss
        result = redis_client.get(f"{base_key}:miss")
        assert result is None
        logger.info("üìä Cache MISS simulado")
        
        # Simular cache hit
        redis_client.set(f"{base_key}:hit", "cached_value")
        result = redis_client.get(f"{base_key}:hit")
        assert result == "cached_value"
        logger.info("üìä Cache HIT simulado")
        
        logger.success("‚úÖ Cache Hit/Miss Pattern: OK")


# ============================================================================
# TESTES DE OPENMETEO API
# ============================================================================

class TestOpenMeteoIntegration:
    """Testes de integra√ß√£o com API OpenMeteo."""
    
    def test_get_elevation_with_cache(self, redis_client, sample_locations):
        """Testa fun√ß√£o get_openmeteo_elevation() com cache Redis."""
        location = sample_locations["ja√∫_sp"]
        
        # Limpar cache anterior
        cache_key = f"elevation:{location['lat']}:{location['long']}"
        redis_client.delete(cache_key)
        
        # Primeira chamada: Cache MISS (vai para API)
        start = time.time()
        elevation1, warnings1 = get_openmeteo_elevation(
            lat=location['lat'],
            long=location['long']
        )
        elapsed_miss = (time.time() - start) * 1000  # ms
        
        assert elevation1 is not None
        assert isinstance(elevation1, float)
        assert -1000 <= elevation1 <= 9000  # Valida√ß√£o do projeto
        logger.info(f"üåê API Call (MISS): {elapsed_miss:.2f}ms - Elevation: {elevation1}m")
        
        # Segunda chamada: Cache HIT (do Redis)
        start = time.time()
        elevation2, warnings2 = get_openmeteo_elevation(
            lat=location['lat'],
            long=location['long']
        )
        elapsed_hit = (time.time() - start) * 1000  # ms
        
        assert elevation2 == elevation1  # Mesma eleva√ß√£o
        logger.info(f"‚ö° Cache Hit (HIT): {elapsed_hit:.2f}ms - Elevation: {elevation2}m")
        
        # Cache deve ser MUITO mais r√°pido (99% improvement esperado)
        assert elapsed_hit < elapsed_miss
        speedup = ((elapsed_miss - elapsed_hit) / elapsed_miss) * 100
        logger.success(f"‚úÖ Cache Performance: {speedup:.1f}% mais r√°pido")
    
    def test_elevation_validation(self, sample_locations):
        """Testa valida√ß√£o de coordenadas e eleva√ß√£o."""
        location = sample_locations["bras√≠lia_df"]
        
        elevation, warnings = get_openmeteo_elevation(
            lat=location['lat'],
            long=location['long']
        )
        
        # Bras√≠lia est√° a ~1000m de altitude
        assert 900 <= elevation <= 1200
        logger.success(f"‚úÖ Elevation Validation: {location['name']} = {elevation}m")
    
    def test_invalid_coordinates(self):
        """Testa tratamento de coordenadas inv√°lidas."""
        with pytest.raises(ValueError):
            get_openmeteo_elevation(lat=100, long=0)  # Lat > 90
        
        with pytest.raises(ValueError):
            get_openmeteo_elevation(lat=0, long=200)  # Long > 180
        
        logger.success("‚úÖ Invalid Coordinates: Properly rejected")


# ============================================================================
# TESTES DE DADOS GEOESPACIAIS
# ============================================================================

class TestGeospatialData:
    """Testes de arquivos geoespaciais do projeto."""
    
    def test_geojson_files_exist(self, geospatial_files):
        """Verifica se arquivos GeoJSON existem."""
        for name, path in geospatial_files.items():
            if name in ["brasil", "matopiba"]:
                assert path.exists(), f"Arquivo n√£o encontrado: {path}"
                logger.success(f"‚úÖ GeoJSON existe: {name} ({path.name})")
    
    def test_load_brasil_geojson(self, geospatial_files):
        """Testa carregamento do GeoJSON do Brasil."""
        brasil_gdf = gpd.read_file(geospatial_files["brasil"])
        
        assert len(brasil_gdf) > 0
        assert brasil_gdf.crs is not None
        assert "geometry" in brasil_gdf.columns
        
        # Brasil tem 27 unidades federativas (26 estados + DF)
        assert len(brasil_gdf) == 27
        
        logger.success(f"‚úÖ Brasil GeoJSON: {len(brasil_gdf)} UFs carregadas")
    
    def test_load_matopiba_geojson(self, geospatial_files):
        """Testa carregamento do GeoJSON do MATOPIBA."""
        matopiba_gdf = gpd.read_file(geospatial_files["matopiba"])
        
        assert len(matopiba_gdf) > 0
        assert matopiba_gdf.crs is not None
        assert "geometry" in matopiba_gdf.columns
        
        logger.success(f"‚úÖ MATOPIBA GeoJSON: {len(matopiba_gdf)} feature(s)")
    
    def test_load_cities_csv(self, geospatial_files):
        """Testa carregamento do CSV de cidades."""
        cities_df = pd.read_csv(geospatial_files["cities"])
        
        assert len(cities_df) > 0
        assert "LATITUDE" in cities_df.columns
        assert "LONGITUDE" in cities_df.columns
        
        # Projeto tem 337 cidades MATOPIBA
        assert len(cities_df) == 337
        
        logger.success(f"Cities CSV: {len(cities_df)} cidades carregadas")
    
    def test_cities_coordinates_valid(self, geospatial_files):
        """Valida coordenadas das cidades."""
        cities_df = pd.read_csv(geospatial_files["cities"])
        
        # Verificar ranges v√°lidos
        assert cities_df["LATITUDE"].between(-90, 90).all()
        assert cities_df["LONGITUDE"].between(-180, 180).all()
        
        # MATOPIBA est√° no Brasil (aproximadamente)
        assert cities_df["LATITUDE"].between(-20, 0).all()  # Latitude
        assert cities_df["LONGITUDE"].between(-60, -35).all()  # Longitude
        
        logger.success("Cities Coordinates: Valida√ß√£o OK")


# ============================================================================
# TESTES DE INTEGRA√á√ÉO COMPLETA
# ============================================================================

class TestFullIntegration:
    """Testes de integra√ß√£o end-to-end."""
    
    def test_full_workflow_map_click_to_elevation(
        self, 
        redis_client, 
        postgres_connection,
        sample_locations
    ):
        """
        Simula workflow completo:
        1. Usu√°rio clica no mapa (coordenadas)
        2. Sistema busca eleva√ß√£o (API + Cache)
        3. Dados s√£o exibidos
        """
        location = sample_locations["palmas_to"]
        
        logger.info(f"üó∫Ô∏è Simulando clique no mapa: {location['name']}")
        logger.info(f"Coordenadas: {location['lat']}, {location['long']}")
        
        # Step 1: Buscar eleva√ß√£o (com cache)
        elevation, warnings = get_openmeteo_elevation(
            lat=location['lat'],
            long=location['long']
        )
        
        assert elevation is not None
        logger.info(f"‚õ∞Ô∏è Eleva√ß√£o: {elevation}m")
        
        # Step 2: Verificar se foi cacheado
        cache_key = f"elevation:{location['lat']}:{location['long']}"
        cached = redis_client.get(cache_key)
        assert cached is not None
        logger.info(f"üíæ Cache: Dados salvos no Redis")
        
        # Step 3: Verificar TTL (24h = 86400s)
        ttl = redis_client.ttl(cache_key)
        assert 0 < ttl <= 86400
        logger.info(f"‚è±Ô∏è TTL: {ttl}s restantes")
        
        logger.success(f"‚úÖ Full Workflow: {location['name']} OK")
    
    def test_multiple_locations_batch(
        self,
        redis_client,
        sample_locations
    ):
        """Testa busca de eleva√ß√£o para m√∫ltiplas localiza√ß√µes."""
        results = {}
        
        for name, loc in sample_locations.items():
            elevation, warnings = get_openmeteo_elevation(
                lat=loc['lat'],
                long=loc['long']
            )
            results[name] = elevation
            logger.info(f"   üìç {name}: {elevation}m")
        
        # Todas devem ter eleva√ß√£o v√°lida
        assert all(elev is not None for elev in results.values())
        assert all(-1000 <= elev <= 9000 for elev in results.values())
        
        logger.success(f"‚úÖ Batch Processing: {len(results)} localiza√ß√µes OK")
    
    def test_database_integration(self, postgres_connection):
        """Testa integra√ß√£o com PostgreSQL (criar tabela de teste)."""
        cursor = postgres_connection.cursor()
        
        # Criar tabela de teste
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS test_locations (
                id SERIAL PRIMARY KEY,
                name VARCHAR(100),
                lat FLOAT,
                long FLOAT,
                elevation FLOAT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        postgres_connection.commit()
        
        # Inserir dados de teste
        cursor.execute("""
            INSERT INTO test_locations (name, lat, long, elevation)
            VALUES (%s, %s, %s, %s)
        """, ("Test Location", -15.7942, -47.8822, 1000.0))
        postgres_connection.commit()
        
        # Verificar inser√ß√£o
        cursor.execute("SELECT COUNT(*) FROM test_locations")
        count = cursor.fetchone()[0]
        assert count > 0
        
        # Cleanup
        cursor.execute("DROP TABLE IF EXISTS test_locations")
        postgres_connection.commit()
        cursor.close()
        
        logger.success("Database Integration: CREATE/INSERT/DROP OK")


# ============================================================================
# TESTES DE PERFORMANCE
# ============================================================================

class TestPerformance:
    """Testes de performance e benchmarking."""
    
    def test_cache_performance_comparison(
        self,
        redis_client,
        sample_locations
    ):
        """Compara performance API vs Cache."""
        location = sample_locations["cuiab√°_mt"]
        cache_key = f"elevation:{location['lat']}:{location['long']}"
        
        # Limpar cache
        redis_client.delete(cache_key)
        
        # Benchmark: API Call (primeira chamada)
        api_times = []
        for _ in range(3):
            redis_client.delete(cache_key)  # For√ßar cache miss
            start = time.time()
            get_openmeteo_elevation(lat=location['lat'], long=location['long'])
            api_times.append((time.time() - start) * 1000)
        
        avg_api = sum(api_times) / len(api_times)
        logger.info(f"   üåê API Average: {avg_api:.2f}ms")
        
        # Benchmark: Cache Hit (chamadas subsequentes)
        cache_times = []
        for _ in range(10):
            start = time.time()
            get_openmeteo_elevation(lat=location['lat'], long=location['long'])
            cache_times.append((time.time() - start) * 1000)
        
        avg_cache = sum(cache_times) / len(cache_times)
        logger.info(f"   ‚ö° Cache Average: {avg_cache:.2f}ms")
        
        # Cache deve ser mais r√°pido que API (m√≠nimo 2x)
        speedup = avg_api / avg_cache
        assert speedup > 2  # No m√≠nimo 2x mais r√°pido
        
        logger.success(
            f"Performance: Cache √© {speedup:.1f}x mais r√°pido que API"
        )
    
    def test_redis_throughput(self, redis_client):
        """Testa throughput do Redis (opera√ß√µes por segundo)."""
        num_operations = 1000
        
        start = time.time()
        for i in range(num_operations):
            redis_client.set(f"test:throughput:{i}", f"value_{i}")
        elapsed = time.time() - start
        
        ops_per_sec = num_operations / elapsed
        logger.success(f"‚úÖ Redis Throughput: {ops_per_sec:.0f} ops/sec")


# ============================================================================
# RELAT√ìRIO FINAL
# ============================================================================

def test_generate_integration_report(
    redis_client,
    postgres_connection,
    geospatial_files
):
    """
    Gera relat√≥rio completo de integra√ß√£o.
    Este teste sempre passa e serve para documenta√ß√£o.
    """
    report = []
    report.append("\n" + "="*70)
    report.append("üìä RELAT√ìRIO DE INTEGRA√á√ÉO - EVAonline")
    report.append("="*70)
    
    # Redis Info
    try:
        redis_info = redis_client.info()
        report.append(f"\nüî¥ REDIS:")
        report.append(f"   Vers√£o: {redis_info.get('redis_version', 'N/A')}")
        report.append(f"   Uptime: {redis_info.get('uptime_in_seconds', 0)} segundos")
        report.append(f"   Conex√µes: {redis_info.get('connected_clients', 0)}")
        report.append(f"   Mem√≥ria usada: {redis_info.get('used_memory_human', 'N/A')}")
        report.append(f"   Keys: {redis_client.dbsize()}")
    except Exception as e:
        report.append(f"\nüî¥ REDIS: Erro ao obter info - {e}")
    
    # PostgreSQL Info
    try:
        with postgres_connection.connect() as conn:
            result = conn.execute(text("SELECT version()"))
            pg_version = result.fetchone()[0].split(',')[0]
            report.append(f"\nüêò POSTGRESQL:")
            report.append(f"   {pg_version}")
            
            result = conn.execute(text("SELECT PostGIS_version()"))
            postgis_version = result.fetchone()[0]
            report.append(f"   PostGIS: {postgis_version}")
            
            result = conn.execute(text("""
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = 'public'
            """))
            table_count = result.fetchone()[0]
            report.append(f"   Tabelas: {table_count}")
    except Exception as e:
        report.append(f"\nüêò POSTGRESQL: Erro ao obter info - {e}")
    
    # Arquivos Geoespaciais
    report.append(f"\nüó∫Ô∏è  ARQUIVOS GEOESPACIAIS:")
    for name, path in geospatial_files.items():
        if path.exists():
            size = path.stat().st_size / 1024  # KB
            report.append(f"   ‚úÖ {name}: {path.name} ({size:.1f} KB)")
        else:
            report.append(f"   ‚ùå {name}: N√ÉO ENCONTRADO")
    
    # Docker Info
    report.append(f"\nüê≥ DOCKER:")
    docker_compose = Path(__file__).parent.parent.parent / "docker-compose.yml"
    if docker_compose.exists():
        report.append(f"   ‚úÖ docker-compose.yml encontrado")
    
    report.append("\n" + "="*70)
    report.append("‚úÖ INTEGRA√á√ÉO COMPLETA: TODOS OS SISTEMAS OPERACIONAIS")
    report.append("="*70 + "\n")
    
    # Print report
    for line in report:
        logger.info(line)
    
    assert True  # Sempre passa
