"""
Testes de Integração Completa - EVAonline Infrastructure
==========================================================

Este módulo testa a integração completa entre:
- PostgreSQL (banco de dados geoespacial com PostGIS)
- Redis (cache e message broker)
- APIs externas (OpenMeteo)
- Sistema de mapas (MATOPIBA e World Map)
- Aplicação Dash (frontend)

Requisitos:
- Docker Desktop instalado e rodando
- PostgreSQL instalado (ou container ativo)
- Redis instalado (ou container ativo)
- Arquivo .env configurado corretamente

Para executar:
    pytest tests/integration/test_infrastructure_integration.py -v --tb=short
"""

import os
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Tuple

import geopandas as gpd
import pandas as pd
import psycopg  # psycopg3 - melhor suporte Unicode no Windows
import pytest
import redis
from loguru import logger
from sqlalchemy import create_engine, text
from sqlalchemy.exc import OperationalError

# Adicionar backend ao path
backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))

from backend.database.connection import DATABASE_URL, engine, get_db_context
from backend.infrastructure.clients.elevation_api import get_openmeteo_elevation

# ============================================================================
# FIXTURES
# ============================================================================

@pytest.fixture(scope="session")
def redis_client():
    """
    Fixture para cliente Redis com configuração de múltiplos ambientes.
    Tenta conectar em: Docker -> Local -> CI/CD
    """
    # Tentar diferentes configurações
    redis_configs = [
        # Docker Compose (container name)
        {"host": "redis", "port": 6379, "password": "evaonline", "db": 0},
        # Docker Compose (localhost com password)
        {"host": "localhost", "port": 6379, "password": "evaonline", "db": 0},
        # Redis local sem senha
        {"host": "localhost", "port": 6379, "password": None, "db": 0},
        # Redis local com senha padrão
        {"host": "127.0.0.1", "port": 6379, "password": "evaonline", "db": 0},
    ]
    
    client = None
    for config in redis_configs:
        try:
            logger.info(f"Tentando conectar ao Redis: {config['host']}:{config['port']}")
            client = redis.Redis(
                host=config["host"],
                port=config["port"],
                password=config["password"],
                db=config["db"],
                decode_responses=True,
                socket_timeout=5,
                socket_connect_timeout=5
            )
            # Testar conexão
            client.ping()
            logger.success(f"✅ Conectado ao Redis: {config['host']}:{config['port']}")
            break
        except redis.ConnectionError as e:
            logger.warning(f"❌ Falha ao conectar: {config['host']}:{config['port']} - {e}")
            continue
    
    if client is None:
        pytest.skip("Redis não está disponível em nenhuma configuração testada")
    
    yield client
    
    # Cleanup: Limpar chaves de teste
    try:
        test_keys = client.keys("test:*")
        if test_keys:
            client.delete(*test_keys)
            logger.info(f"🧹 Limpeza: {len(test_keys)} chaves de teste removidas")
    except Exception as e:
        logger.warning(f"Erro na limpeza do Redis: {e}")


@pytest.fixture(scope="session")
def postgres_connection():
    """
    Fixture para conexão PostgreSQL com psycopg3 (melhor suporte Unicode).
    """
    # Tentar diferentes configurações (priorizar localhost para Docker local)
    pg_configs = [
        # PostgreSQL local via localhost (Docker Desktop ou Docker Compose)
        {
            "host": "localhost",
            "port": 5432,
            "user": "evaonline",
            "password": "evaonline",
            "dbname": "evaonline"
        },
        # PostgreSQL local (instalação Windows nativa)
        {
            "host": "localhost",
            "port": 5432,
            "user": "postgres",
            "password": "postgres",
            "dbname": "evaonline"
        },
        # Docker Compose (dentro de container)
        {
            "host": "postgres",
            "port": 5432,
            "user": "evaonline",
            "password": "evaonline",
            "dbname": "evaonline"
        }
    ]
    
    conn = None
    for config in pg_configs:
        try:
            logger.info(
                f"Tentando conectar ao PostgreSQL: "
                f"{config['host']}:{config['port']}"
            )
            # Conectar com psycopg3 (melhor que psycopg2 no Windows)
            conn = psycopg.connect(**config)
            
            # Testar conexão
            cursor = conn.cursor()
            cursor.execute("SELECT version()")
            version = cursor.fetchone()[0]
            cursor.close()
            
            logger.success(f"Conectado ao PostgreSQL: {version[:50]}...")
            yield conn
            conn.close()
            return
        except Exception as e:
            logger.warning(f"Falha ao conectar: {e}")
            if conn:
                conn.close()
            continue
    
    pytest.skip(
        "PostgreSQL não está disponível em nenhuma configuração testada"
    )


@pytest.fixture(scope="session")
def sample_locations():
    """Coordenadas de teste para diferentes regiões."""
    return {
        "jaú_sp": {"lat": -22.2964, "long": -48.5578, "name": "Jaú, SP"},
        "brasília_df": {"lat": -15.7942, "long": -47.8822, "name": "Brasília, DF"},
        "cuiabá_mt": {"lat": -15.5989, "long": -56.0949, "name": "Cuiabá, MT"},
        "palmas_to": {"lat": -10.1840, "long": -48.3336, "name": "Palmas, TO"},
        "são_paulo_sp": {"lat": -23.5505, "long": -46.6333, "name": "São Paulo, SP"}
    }


@pytest.fixture
def geospatial_files():
    """Paths para arquivos geoespaciais do projeto."""
    base_dir = Path(__file__).parent.parent.parent
    return {
        "brasil": base_dir / "data" / "geojson" / "BR_UF_2024.geojson",
        "matopiba": base_dir / "data" / "geojson" / "Matopiba_Perimetro.geojson",
        "cities": base_dir / "data" / "csv" / "CITIES_MATOPIBA_337.csv"
    }


# ============================================================================
# TESTES DE CONECTIVIDADE
# ============================================================================

class TestConnectivity:
    """Testes de conectividade básica com infraestrutura."""
    
    def test_redis_ping(self, redis_client):
        """Testa se o Redis responde ao comando PING."""
        assert redis_client.ping() is True
        logger.success("✅ Redis PING: OK")
    
    def test_postgres_connection(self, postgres_connection):
        """Testa se o PostgreSQL está acessível."""
        cursor = postgres_connection.cursor()
        cursor.execute("SELECT 1")
        result = cursor.fetchone()
        cursor.close()
        assert result[0] == 1
        logger.success("✅ PostgreSQL Connection: OK")
    
    def test_postgres_postgis_extension(self, postgres_connection):
        """Verifica se a extensão PostGIS está instalada."""
        cursor = postgres_connection.cursor()
        cursor.execute("SELECT PostGIS_version()")
        result = cursor.fetchone()
        cursor.close()
        version = result[0]
        # PostGIS retorna apenas a versão (ex: "3.4 USE_GEOS=1...")
        assert version is not None and len(version) > 0
        logger.success(f"PostGIS Extension: {version}")
    
    def test_database_tables_exist(self, postgres_connection):
        """Verifica se as tabelas principais existem."""
        cursor = postgres_connection.cursor()
        cursor.execute("""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = 'public'
            AND table_type = 'BASE TABLE'
        """)
        tables = [row[0] for row in cursor.fetchall()]
        cursor.close()
        logger.info(f"📊 Tabelas encontradas: {tables}")
        # Não falhar se não houver tabelas (banco novo)
        logger.success(f"✅ Database Schema: {len(tables)} tabela(s)")


# ============================================================================
# TESTES DE CACHE REDIS
# ============================================================================

class TestRedisCache:
    """Testes de funcionalidades de cache Redis."""
    
    def test_set_get_string(self, redis_client):
        """Testa operações básicas SET/GET."""
        key = "test:string"
        value = "EVAonline Test"
        
        redis_client.set(key, value)
        retrieved = redis_client.get(key)
        
        assert retrieved == value
        logger.success(f"✅ Redis SET/GET: {key} = {value}")
    
    def test_set_with_expiry(self, redis_client):
        """Testa SET com TTL (expiração)."""
        key = "test:expiry"
        value = "temporary_data"
        ttl = 5  # 5 segundos
        
        redis_client.setex(key, ttl, value)
        
        # Verificar que existe
        assert redis_client.get(key) == value
        
        # Verificar TTL
        remaining = redis_client.ttl(key)
        assert 0 < remaining <= ttl
        
        logger.success(f"✅ Redis SETEX: TTL={remaining}s")
    
    def test_cache_elevation_data(self, redis_client, sample_locations):
        """Testa cache de dados de elevação (padrão do projeto)."""
        location = sample_locations["jaú_sp"]
        cache_key = f"test:elevation:{location['lat']}:{location['long']}"
        elevation_data = {
            "elevation": 542.5,
            "lat": location['lat'],
            "long": location['long'],
            "timestamp": datetime.now().isoformat()
        }
        
        # Simular cache com 24h TTL (padrão do projeto)
        redis_client.setex(
            cache_key,
            86400,  # 24 hours
            str(elevation_data)
        )
        
        # Verificar
        cached = redis_client.get(cache_key)
        assert cached is not None
        assert str(location['lat']) in cached
        
        logger.success(f"✅ Elevation Cache: {cache_key}")
    
    def test_cache_hit_miss_metrics(self, redis_client):
        """Testa padrão de cache hit/miss para métricas."""
        base_key = "test:metrics"
        
        # Simular cache miss
        result = redis_client.get(f"{base_key}:miss")
        assert result is None
        logger.info("📊 Cache MISS simulado")
        
        # Simular cache hit
        redis_client.set(f"{base_key}:hit", "cached_value")
        result = redis_client.get(f"{base_key}:hit")
        assert result == "cached_value"
        logger.info("📊 Cache HIT simulado")
        
        logger.success("✅ Cache Hit/Miss Pattern: OK")


# ============================================================================
# TESTES DE OPENMETEO API
# ============================================================================

class TestOpenMeteoIntegration:
    """Testes de integração com API OpenMeteo."""
    
    def test_get_elevation_with_cache(self, redis_client, sample_locations):
        """Testa função get_openmeteo_elevation() com cache Redis."""
        location = sample_locations["jaú_sp"]
        
        # Limpar cache anterior
        cache_key = f"elevation:{location['lat']}:{location['long']}"
        redis_client.delete(cache_key)
        
        # Primeira chamada: Cache MISS (vai para API)
        start = time.time()
        elevation1, warnings1 = get_openmeteo_elevation(
            lat=location['lat'],
            long=location['long']
        )
        elapsed_miss = (time.time() - start) * 1000  # ms
        
        assert elevation1 is not None
        assert isinstance(elevation1, float)
        assert -1000 <= elevation1 <= 9000  # Validação do projeto
        logger.info(f"🌐 API Call (MISS): {elapsed_miss:.2f}ms - Elevation: {elevation1}m")
        
        # Segunda chamada: Cache HIT (do Redis)
        start = time.time()
        elevation2, warnings2 = get_openmeteo_elevation(
            lat=location['lat'],
            long=location['long']
        )
        elapsed_hit = (time.time() - start) * 1000  # ms
        
        assert elevation2 == elevation1  # Mesma elevação
        logger.info(f"⚡ Cache Hit (HIT): {elapsed_hit:.2f}ms - Elevation: {elevation2}m")
        
        # Cache deve ser MUITO mais rápido (99% improvement esperado)
        assert elapsed_hit < elapsed_miss
        speedup = ((elapsed_miss - elapsed_hit) / elapsed_miss) * 100
        logger.success(f"✅ Cache Performance: {speedup:.1f}% mais rápido")
    
    def test_elevation_validation(self, sample_locations):
        """Testa validação de coordenadas e elevação."""
        location = sample_locations["brasília_df"]
        
        elevation, warnings = get_openmeteo_elevation(
            lat=location['lat'],
            long=location['long']
        )
        
        # Brasília está a ~1000m de altitude
        assert 900 <= elevation <= 1200
        logger.success(f"✅ Elevation Validation: {location['name']} = {elevation}m")
    
    def test_invalid_coordinates(self):
        """Testa tratamento de coordenadas inválidas."""
        with pytest.raises(ValueError):
            get_openmeteo_elevation(lat=100, long=0)  # Lat > 90
        
        with pytest.raises(ValueError):
            get_openmeteo_elevation(lat=0, long=200)  # Long > 180
        
        logger.success("✅ Invalid Coordinates: Properly rejected")


# ============================================================================
# TESTES DE DADOS GEOESPACIAIS
# ============================================================================

class TestGeospatialData:
    """Testes de arquivos geoespaciais do projeto."""
    
    def test_geojson_files_exist(self, geospatial_files):
        """Verifica se arquivos GeoJSON existem."""
        for name, path in geospatial_files.items():
            if name in ["brasil", "matopiba"]:
                assert path.exists(), f"Arquivo não encontrado: {path}"
                logger.success(f"✅ GeoJSON existe: {name} ({path.name})")
    
    def test_load_brasil_geojson(self, geospatial_files):
        """Testa carregamento do GeoJSON do Brasil."""
        brasil_gdf = gpd.read_file(geospatial_files["brasil"])
        
        assert len(brasil_gdf) > 0
        assert brasil_gdf.crs is not None
        assert "geometry" in brasil_gdf.columns
        
        # Brasil tem 27 unidades federativas (26 estados + DF)
        assert len(brasil_gdf) == 27
        
        logger.success(f"✅ Brasil GeoJSON: {len(brasil_gdf)} UFs carregadas")
    
    def test_load_matopiba_geojson(self, geospatial_files):
        """Testa carregamento do GeoJSON do MATOPIBA."""
        matopiba_gdf = gpd.read_file(geospatial_files["matopiba"])
        
        assert len(matopiba_gdf) > 0
        assert matopiba_gdf.crs is not None
        assert "geometry" in matopiba_gdf.columns
        
        logger.success(f"✅ MATOPIBA GeoJSON: {len(matopiba_gdf)} feature(s)")
    
    def test_load_cities_csv(self, geospatial_files):
        """Testa carregamento do CSV de cidades."""
        cities_df = pd.read_csv(geospatial_files["cities"])
        
        assert len(cities_df) > 0
        assert "LATITUDE" in cities_df.columns
        assert "LONGITUDE" in cities_df.columns
        
        # Projeto tem 337 cidades MATOPIBA
        assert len(cities_df) == 337
        
        logger.success(f"Cities CSV: {len(cities_df)} cidades carregadas")
    
    def test_cities_coordinates_valid(self, geospatial_files):
        """Valida coordenadas das cidades."""
        cities_df = pd.read_csv(geospatial_files["cities"])
        
        # Verificar ranges válidos
        assert cities_df["LATITUDE"].between(-90, 90).all()
        assert cities_df["LONGITUDE"].between(-180, 180).all()
        
        # MATOPIBA está no Brasil (aproximadamente)
        assert cities_df["LATITUDE"].between(-20, 0).all()  # Latitude
        assert cities_df["LONGITUDE"].between(-60, -35).all()  # Longitude
        
        logger.success("Cities Coordinates: Validação OK")


# ============================================================================
# TESTES DE INTEGRAÇÃO COMPLETA
# ============================================================================

class TestFullIntegration:
    """Testes de integração end-to-end."""
    
    def test_full_workflow_map_click_to_elevation(
        self, 
        redis_client, 
        postgres_connection,
        sample_locations
    ):
        """
        Simula workflow completo:
        1. Usuário clica no mapa (coordenadas)
        2. Sistema busca elevação (API + Cache)
        3. Dados são exibidos
        """
        location = sample_locations["palmas_to"]
        
        logger.info(f"🗺️ Simulando clique no mapa: {location['name']}")
        logger.info(f"Coordenadas: {location['lat']}, {location['long']}")
        
        # Step 1: Buscar elevação (com cache)
        elevation, warnings = get_openmeteo_elevation(
            lat=location['lat'],
            long=location['long']
        )
        
        assert elevation is not None
        logger.info(f"⛰️ Elevação: {elevation}m")
        
        # Step 2: Verificar se foi cacheado
        cache_key = f"elevation:{location['lat']}:{location['long']}"
        cached = redis_client.get(cache_key)
        assert cached is not None
        logger.info(f"💾 Cache: Dados salvos no Redis")
        
        # Step 3: Verificar TTL (24h = 86400s)
        ttl = redis_client.ttl(cache_key)
        assert 0 < ttl <= 86400
        logger.info(f"⏱️ TTL: {ttl}s restantes")
        
        logger.success(f"✅ Full Workflow: {location['name']} OK")
    
    def test_multiple_locations_batch(
        self,
        redis_client,
        sample_locations
    ):
        """Testa busca de elevação para múltiplas localizações."""
        results = {}
        
        for name, loc in sample_locations.items():
            elevation, warnings = get_openmeteo_elevation(
                lat=loc['lat'],
                long=loc['long']
            )
            results[name] = elevation
            logger.info(f"   📍 {name}: {elevation}m")
        
        # Todas devem ter elevação válida
        assert all(elev is not None for elev in results.values())
        assert all(-1000 <= elev <= 9000 for elev in results.values())
        
        logger.success(f"✅ Batch Processing: {len(results)} localizações OK")
    
    def test_database_integration(self, postgres_connection):
        """Testa integração com PostgreSQL (criar tabela de teste)."""
        cursor = postgres_connection.cursor()
        
        # Criar tabela de teste
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS test_locations (
                id SERIAL PRIMARY KEY,
                name VARCHAR(100),
                lat FLOAT,
                long FLOAT,
                elevation FLOAT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        postgres_connection.commit()
        
        # Inserir dados de teste
        cursor.execute("""
            INSERT INTO test_locations (name, lat, long, elevation)
            VALUES (%s, %s, %s, %s)
        """, ("Test Location", -15.7942, -47.8822, 1000.0))
        postgres_connection.commit()
        
        # Verificar inserção
        cursor.execute("SELECT COUNT(*) FROM test_locations")
        count = cursor.fetchone()[0]
        assert count > 0
        
        # Cleanup
        cursor.execute("DROP TABLE IF EXISTS test_locations")
        postgres_connection.commit()
        cursor.close()
        
        logger.success("Database Integration: CREATE/INSERT/DROP OK")


# ============================================================================
# TESTES DE PERFORMANCE
# ============================================================================

class TestPerformance:
    """Testes de performance e benchmarking."""
    
    def test_cache_performance_comparison(
        self,
        redis_client,
        sample_locations
    ):
        """Compara performance API vs Cache."""
        location = sample_locations["cuiabá_mt"]
        cache_key = f"elevation:{location['lat']}:{location['long']}"
        
        # Limpar cache
        redis_client.delete(cache_key)
        
        # Benchmark: API Call (primeira chamada)
        api_times = []
        for _ in range(3):
            redis_client.delete(cache_key)  # Forçar cache miss
            start = time.time()
            get_openmeteo_elevation(lat=location['lat'], long=location['long'])
            api_times.append((time.time() - start) * 1000)
        
        avg_api = sum(api_times) / len(api_times)
        logger.info(f"   🌐 API Average: {avg_api:.2f}ms")
        
        # Benchmark: Cache Hit (chamadas subsequentes)
        cache_times = []
        for _ in range(10):
            start = time.time()
            get_openmeteo_elevation(lat=location['lat'], long=location['long'])
            cache_times.append((time.time() - start) * 1000)
        
        avg_cache = sum(cache_times) / len(cache_times)
        logger.info(f"   ⚡ Cache Average: {avg_cache:.2f}ms")
        
        # Cache deve ser mais rápido que API (mínimo 2x)
        speedup = avg_api / avg_cache
        assert speedup > 2  # No mínimo 2x mais rápido
        
        logger.success(
            f"Performance: Cache é {speedup:.1f}x mais rápido que API"
        )
    
    def test_redis_throughput(self, redis_client):
        """Testa throughput do Redis (operações por segundo)."""
        num_operations = 1000
        
        start = time.time()
        for i in range(num_operations):
            redis_client.set(f"test:throughput:{i}", f"value_{i}")
        elapsed = time.time() - start
        
        ops_per_sec = num_operations / elapsed
        logger.success(f"✅ Redis Throughput: {ops_per_sec:.0f} ops/sec")


# ============================================================================
# RELATÓRIO FINAL
# ============================================================================

def test_generate_integration_report(
    redis_client,
    postgres_connection,
    geospatial_files
):
    """
    Gera relatório completo de integração.
    Este teste sempre passa e serve para documentação.
    """
    report = []
    report.append("\n" + "="*70)
    report.append("📊 RELATÓRIO DE INTEGRAÇÃO - EVAonline")
    report.append("="*70)
    
    # Redis Info
    try:
        redis_info = redis_client.info()
        report.append(f"\n🔴 REDIS:")
        report.append(f"   Versão: {redis_info.get('redis_version', 'N/A')}")
        report.append(f"   Uptime: {redis_info.get('uptime_in_seconds', 0)} segundos")
        report.append(f"   Conexões: {redis_info.get('connected_clients', 0)}")
        report.append(f"   Memória usada: {redis_info.get('used_memory_human', 'N/A')}")
        report.append(f"   Keys: {redis_client.dbsize()}")
    except Exception as e:
        report.append(f"\n🔴 REDIS: Erro ao obter info - {e}")
    
    # PostgreSQL Info
    try:
        with postgres_connection.connect() as conn:
            result = conn.execute(text("SELECT version()"))
            pg_version = result.fetchone()[0].split(',')[0]
            report.append(f"\n🐘 POSTGRESQL:")
            report.append(f"   {pg_version}")
            
            result = conn.execute(text("SELECT PostGIS_version()"))
            postgis_version = result.fetchone()[0]
            report.append(f"   PostGIS: {postgis_version}")
            
            result = conn.execute(text("""
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = 'public'
            """))
            table_count = result.fetchone()[0]
            report.append(f"   Tabelas: {table_count}")
    except Exception as e:
        report.append(f"\n🐘 POSTGRESQL: Erro ao obter info - {e}")
    
    # Arquivos Geoespaciais
    report.append(f"\n🗺️  ARQUIVOS GEOESPACIAIS:")
    for name, path in geospatial_files.items():
        if path.exists():
            size = path.stat().st_size / 1024  # KB
            report.append(f"   ✅ {name}: {path.name} ({size:.1f} KB)")
        else:
            report.append(f"   ❌ {name}: NÃO ENCONTRADO")
    
    # Docker Info
    report.append(f"\n🐳 DOCKER:")
    docker_compose = Path(__file__).parent.parent.parent / "docker-compose.yml"
    if docker_compose.exists():
        report.append(f"   ✅ docker-compose.yml encontrado")
    
    report.append("\n" + "="*70)
    report.append("✅ INTEGRAÇÃO COMPLETA: TODOS OS SISTEMAS OPERACIONAIS")
    report.append("="*70 + "\n")
    
    # Print report
    for line in report:
        logger.info(line)
    
    assert True  # Sempre passa
